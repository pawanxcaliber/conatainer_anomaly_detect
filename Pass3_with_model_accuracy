import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest # Import IsolationForest
from sklearn.svm import OneClassSVM
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score # Import metrics

# --- 1. Data Preparation Function (from previous steps, consolidated) ---
def prepare_data_for_ml(
    processed_data_file='processed_telemetry_data.csv',
    test_size=0.15, # 15% for test
    val_size=0.15,  # 15% for validation
    random_seed=42,
    scaler_output_path='standard_scaler.joblib' # Path to save the fitted scaler
):
    """
    Loads processed data, splits it into training, validation, and test sets,
    scales the features, and saves the fitted scaler.

    Args:
        processed_data_file (str): Path to the processed telemetry data CSV.
        test_size (float): Proportion of the dataset to include in the test split.
        val_size (float): Proportion of the dataset to include in the validation split.
        random_seed (int): Seed for random operations for reproducibility.
        scaler_output_path (str): Path to save the fitted StandardScaler object.

    Returns:
        tuple: (X_train_scaled_df, X_val_scaled_df, X_test_scaled_df, df_processed_original, features_columns)
               The scaled training, validation, test feature sets, original processed DF, and feature columns list.
        None: If an error occurs.
    """
    try:
        # --- Load the processed data ---
        df_processed = pd.read_csv(processed_data_file)
        print(f"Processed data loaded successfully from {processed_data_file}")
        print(f"Shape of loaded data: {df_processed.shape}")

        df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'])

        # --- Identify features (X) ---
        columns_to_exclude = ['Timestamp', 'ContainerID', 'ServiceName']
        features_columns = [col for col in df_processed.columns if col not in columns_to_exclude]
        X = df_processed[features_columns]
        print(f"\nFeatures (X) DataFrame shape: {X.shape}")

        # --- Calculate test and validation sizes from total remaining ---
        if (test_size + val_size) >= 1.0:
            raise ValueError("test_size + val_size must be less than 1.0")

        temp_size = test_size + val_size
        X_train, X_temp = train_test_split(X, test_size=temp_size, random_state=random_seed)

        val_size_relative_to_temp = val_size / temp_size
        X_val, X_test = train_test_split(X_temp, test_size=val_size_relative_to_temp, random_state=random_seed)

        print("\nData Split Complete:")
        print(f"X_train shape: {X_train.shape}")
        print(f"X_val shape: {X_val.shape}")
        print(f"X_test shape: {X_test.shape}")

        # --- Feature Scaling/Normalization ---
        scaler = StandardScaler()
        scaler.fit(X_train) # Fit ONLY on the training data

        X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=features_columns, index=X_train.index)
        X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=features_columns, index=X_val.index)
        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=features_columns, index=X_test.index)

        print("\nFeature Scaling Complete.")

        # --- Save the fitted scaler ---
        joblib.dump(scaler, scaler_output_path)
        print(f"\nFitted StandardScaler saved to {scaler_output_path}")

        # Return original df_processed too, for easier inspection of anomalies later
        return X_train_scaled, X_val_scaled, X_test_scaled, df_processed, features_columns

    except FileNotFoundError:
        print(f"Error: The file '{processed_data_file}' was not found. Please ensure the preprocessing script was run and the file exists.")
        return None, None, None, None, None
    except Exception as e:
        print(f"An error occurred during data preparation: {e}")
        return None, None, None, None, None


# --- Define the anomaly detection function for deployment ---
def detect_anomaly_realtime(
    new_data_point: pd.DataFrame,
    scaler_path: str = 'standard_scaler.joblib',
    model_path: str = 'one_class_svm_model.joblib' # Default to OCSVM
) -> tuple:
    """
    Simulates real-time anomaly detection for a single or multiple new data points.

    Args:
        new_data_point (pd.DataFrame): A DataFrame containing the new telemetry data point(s).
                                       Must have the same feature columns as the training data.
        scaler_path (str): Path to the saved StandardScaler object.
        model_path (str): Path to the saved OneClassSVM model object (or IsolationForest model).

    Returns:
        tuple: (prediction, anomaly_score)
               prediction: -1 for anomaly, 1 for normal
               anomaly_score: The decision function score (negative is anomalous for OCSVM, lower for IF)
        None: If an error occurs during loading or prediction.
    """
    try:
        # Load the pre-trained scaler and model
        scaler = joblib.load(scaler_path)
        model = joblib.load(model_path)
        
        # If new_data_point is a Series (single row), convert it to DataFrame (single row, multiple columns)
        if isinstance(new_data_point, pd.Series):
            new_data_point = new_data_point.to_frame().T
        
        # Preprocess the new data point using the loaded scaler
        scaled_data_point = scaler.transform(new_data_point)
        
        scaled_data_point_df = pd.DataFrame(scaled_data_point, columns=new_data_point.columns, index=new_data_point.index)

        prediction = model.predict(scaled_data_point_df)
        anomaly_score = model.decision_function(scaled_data_point_df)

        return prediction, anomaly_score

    except FileNotFoundError as e:
        print(f"Error: Required file not found - {e}")
        return None, None
    except Exception as e:
        print(f"An error occurred during real-time detection: {e}")
        return None, None

# --- Main Execution Block ---
if __name__ == '__main__':
    # Make sure you have 'processed_telemetry_data.csv' generated from previous steps
    # and the models/scalers saved as 'standard_scaler.joblib' and 'one_class_svm_model.joblib'.

    # 1. Prepare the data (load, split, scale, save scaler) - Only needs to run once
    print("\n--- Starting Data Preparation (for model training/saving if not done) ---")
    X_train_scaled, X_val_scaled, X_test_scaled, df_processed_original, features_columns = prepare_data_for_ml(
        processed_data_file='processed_telemetry_data.csv',
        scaler_output_path='standard_scaler.joblib'
    )

    if X_train_scaled is None:
        print("Exiting due to data preparation error. Please check previous logs.")
        exit()

    # --- Define Ground Truth for Evaluation ---
    # Assuming 'malicious-container-x' is the true anomaly source
    # Create a 'true_label' column: 1 for anomaly, 0 for normal
    df_processed_original['true_label'] = (df_processed_original['ContainerID'] == 'malicious-container-x').astype(int)
    print(f"\nGround Truth established: {df_processed_original['true_label'].sum()} true anomalies identified.")


    # --- Training/Loading One-Class SVM Model for Deployment ---
    print("\n--- Training/Loading One-Class SVM Model for Deployment ---")
    ocsvm_model_output_path = 'one_class_svm_model.joblib'
    try:
        ocsvm_model = joblib.load(ocsvm_model_output_path)
        print(f"One-Class SVM model loaded from {ocsvm_model_output_path}.")
    except FileNotFoundError:
        print("One-Class SVM model not found, training new one.")
        ocsvm_model = OneClassSVM(kernel='rbf', nu=0.01, gamma='scale') # Using nu=0.01
        ocsvm_model.fit(X_train_scaled)
        joblib.dump(ocsvm_model, ocsvm_model_output_path)
        print(f"Trained One-Class SVM model saved to {ocsvm_model_output_path}.")
    
    # --- Calculate OCSVM predictions for the entire dataset immediately after model is ready ---
    scaler_for_all = joblib.load('standard_scaler.joblib') # Load scaler to transform full dataset
    X_all_scaled_ocsvm = scaler_for_all.transform(df_processed_original[features_columns])
    df_processed_original['ocsvm_anomaly_score'] = ocsvm_model.decision_function(X_all_scaled_ocsvm)
    df_processed_original['ocsvm_anomaly_prediction'] = ocsvm_model.predict(X_all_scaled_ocsvm)


    # --- Simulating Real-time Anomaly Detection (Phase 3) for OCSVM ---
    print("\n--- Simulating Real-time Anomaly Detection (Phase 3) for OCSVM ---")

    # Re-run simulation outputs without re-loading scaler/model print
    print(f"\n--- Simulating Anomaly: {df_processed_original.loc[4149]['ContainerID']} at {df_processed_original.loc[4149]['Timestamp']} ---")
    # Note: For simulation, we still use detect_anomaly_realtime as it simulates the real-time flow
    pred_ocsvm, score_ocsvm = detect_anomaly_realtime(df_processed_original[features_columns].loc[[4149]], model_path=ocsvm_model_output_path)
    if pred_ocsvm is not None:
        status = "Anomaly Detected!" if pred_ocsvm[0] == -1 else "Normal"
        print(f"Prediction: {status}, Anomaly Score: {score_ocsvm[0]:.4f}")
        print(f"  Original Info: ContainerID={df_processed_original.loc[4149]['ContainerID']}, Service={df_processed_original.loc[4149]['ServiceName']}")

    normal_sample_index = df_processed_original[df_processed_original['ContainerID'] != 'malicious-container-x'].sample(1, random_state=1).index[0]
    print(f"\n--- Simulating Normal: {df_processed_original.loc[normal_sample_index]['ContainerID']} at {df_processed_original.loc[normal_sample_index]['Timestamp']} ---")
    pred_ocsvm, score_ocsvm = detect_anomaly_realtime(df_processed_original[features_columns].loc[[normal_sample_index]], model_path=ocsvm_model_output_path)
    if pred_ocsvm is not None:
        status = "Anomaly Detected!" if pred_ocsvm[0] == -1 else "Normal"
        print(f"Prediction: {status}, Anomaly Score: {score_ocsvm[0]:.4f}")
        print(f"  Original Info: ContainerID={df_processed_original.loc[normal_sample_index]['ContainerID']}, Service={df_processed_original.loc[normal_sample_index]['ServiceName']}")

    custom_anomaly_data = pd.DataFrame([{
        'BytesSentKBps': 5000.0, 'BytesReceivedKBps': 6000.0, 'PacketsDropped': 500.0, 'LatencyMs': 2000.0,
        'ConnectionsActive': 300.0, 'ConnectionFailures': 50.0, 'CPUUtilizationPercent': 150.0,
        'MemoryUsageMB': 5000.0, 'DiskReadKBps': 5000.0, 'DiskWriteKBps': 4000.0,
        'ProcessCount': 100.0, 'TotalLogs': 20.0, 'InfoLogs': 0.0, 'DebugLogs': 0.0,
        'WarningLogs': 0.0, 'ErrorLogs': 10.0, 'CriticalLogs': 10.0
    }], columns=features_columns)
    print("\n--- Simulating a NEW Custom Anomaly ---")
    pred_ocsvm, score_ocsvm = detect_anomaly_realtime(custom_anomaly_data, model_path=ocsvm_model_output_path)
    if pred_ocsvm is not None:
        status = "Anomaly Detected!" if pred_ocsvm[0] == -1 else "Normal"
        print(f"Prediction: {status}, Anomaly Score: {score_ocsvm[0]:.4f}")
        if status == "Anomaly Detected!":
            print("  -> The model successfully flagged our custom-designed anomaly!")

    print("\n--- Phase 3: Deployment & Integration Simulation Complete (OCSVM) ---")


    # --- Generating Grafana-like Output (Static Plot) for OCSVM ---
    print("\n--- Generating Grafana-like Anomaly Visualization (OCSVM) ---")
    try:
        # The predictions and scores for OCSVM are already calculated above, so no need to recalculate here.
        ocsvm_anomalies_df = df_processed_original[df_processed_original['ocsvm_anomaly_prediction'] == -1]

        plt.figure(figsize=(18, 10))
        sns.set_style("whitegrid")

        plt.subplot(2, 1, 1) 
        sns.lineplot(x='Timestamp', y='CPUUtilizationPercent', data=df_processed_original, hue='ContainerID', linewidth=0.8, alpha=0.7, palette='tab10', legend=False)
        plt.scatter(ocsvm_anomalies_df['Timestamp'], ocsvm_anomalies_df['CPUUtilizationPercent'], 
                    color='red', s=50, label='OCSVM Detected Anomaly', zorder=5, marker='X')
        plt.title('CPU Utilization with Detected Anomalies (OCSVM - Grafana-like View)', fontsize=16)
        plt.xlabel('Timestamp', fontsize=12)
        plt.ylabel('CPU Utilization (%)', fontsize=12)
        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
        plt.grid(True)
        plt.tight_layout()

        plt.subplot(2, 1, 2) 
        sns.lineplot(x='Timestamp', y='ocsvm_anomaly_score', data=df_processed_original, hue='ContainerID', linewidth=0.8, alpha=0.7, palette='tab10', legend=False)
        plt.axhline(y=0, color='grey', linestyle='--', label='OCSVM Anomaly Threshold (0)')
        plt.scatter(ocsvm_anomalies_df['Timestamp'], ocsvm_anomalies_df['ocsvm_anomaly_score'], 
                    color='red', s=50, label='OCSVM Detected Anomaly', zorder=5, marker='X')
        plt.title('OCSVM Anomaly Score Over Time', fontsize=16)
        plt.xlabel('Timestamp', fontsize=12)
        plt.ylabel('Anomaly Score (OCSVM Decision Function)', fontsize=12)
        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
        plt.grid(True)
        plt.tight_layout()
        
        plt.suptitle("Simulated Anomaly Dashboard View (OCSVM)", y=1.02, fontsize=20)
        plt.show()

    except Exception as e:
        print(f"An error occurred during OCSVM plotting: {e}")

    # --- Generating Grafana-like Output (Static Plot) for ISOLATION FOREST ---
    print("\n--- Generating Grafana-like Anomaly Visualization (Isolation Forest) ---")

    isolation_forest_model_output_path = 'isolation_forest_model.joblib'
    try:
        # Load scaler
        scaler_for_all = joblib.load('standard_scaler.joblib')

        # Try loading Isolation Forest model, if not found, train it
        try:
            if_model = joblib.load(isolation_forest_model_output_path)
            print(f"Isolation Forest model loaded from {isolation_forest_model_output_path}.")
        except FileNotFoundError:
            print("Isolation Forest model not found, training new one.")
            # Contamination is an important parameter for Isolation Forest, represents expected proportion of outliers
            # A typical value is 0.01 (1%) if not specified by problem domain.
            if_model = IsolationForest(random_state=42, contamination=0.01) # Using contamination=0.01 for IF
            if_model.fit(X_train_scaled) # Fit on scaled training data
            joblib.dump(if_model, isolation_forest_model_output_path)
            print(f"Trained Isolation Forest model saved to {isolation_forest_model_output_path}.")

        # --- Calculate Isolation Forest predictions for the entire dataset immediately after model is ready ---
        X_all_scaled_if = scaler_for_all.transform(df_processed_original[features_columns])
        df_processed_original['if_anomaly_score'] = if_model.decision_function(X_all_scaled_if)
        df_processed_original['if_anomaly_prediction'] = if_model.predict(X_all_scaled_if)
        
        # Identify anomalies based on prediction for IF
        if_anomalies_df = df_processed_original[df_processed_original['if_anomaly_prediction'] == -1]

        # --- Create the Plot for Isolation Forest ---
        plt.figure(figsize=(18, 10))
        sns.set_style("whitegrid")

        # Plot 1: CPU Utilization over time
        plt.subplot(2, 1, 1) 
        sns.lineplot(x='Timestamp', y='CPUUtilizationPercent', data=df_processed_original, hue='ContainerID', linewidth=0.8, alpha=0.7, palette='tab10', legend=False)
        plt.scatter(if_anomalies_df['Timestamp'], if_anomalies_df['CPUUtilizationPercent'], 
                    color='purple', s=50, label='IF Detected Anomaly', zorder=5, marker='X') # Using purple for IF
        plt.title('CPU Utilization with Detected Anomalies (Isolation Forest - Grafana-like View)', fontsize=16)
        plt.xlabel('Timestamp', fontsize=12)
        plt.ylabel('CPU Utilization (%)', fontsize=12)
        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
        plt.grid(True)
        plt.tight_layout()

        plt.subplot(2, 1, 2) 
        sns.lineplot(x='Timestamp', y='if_anomaly_score', data=df_processed_original, hue='ContainerID', linewidth=0.8, alpha=0.7, palette='tab10', legend=False)
        
        # For Isolation Forest, the threshold is typically 0, but can vary based on contamination
        # The decision_function produces values where lower is more anomalous, typically around 0 for normal.
        plt.axhline(y=0, color='grey', linestyle='--', label='IF Anomaly Threshold (0)') 
        
        plt.scatter(if_anomalies_df['Timestamp'], if_anomalies_df['if_anomaly_score'], 
                    color='purple', s=50, label='IF Detected Anomaly', zorder=5, marker='X') # Using purple for IF

        plt.title('Isolation Forest Anomaly Score Over Time', fontsize=16)
        plt.xlabel('Timestamp', fontsize=12)
        plt.ylabel('Anomaly Score (Isolation Forest Decision Function)', fontsize=12)
        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
        plt.grid(True)
        plt.tight_layout()
        
        plt.suptitle("Simulated Anomaly Dashboard View (Isolation Forest)", y=1.02, fontsize=20) # Overall title
        plt.show()

    except Exception as e:
        print(f"An error occurred during Isolation Forest plotting: {e}")


    # --- Quantifying ML Model Performance --- # <--- THIS SECTION HAS BEEN MOVED HERE!
    print("\n--- Quantifying ML Model Performance ---")

    # Evaluate OCSVM Performance
    # Convert OCSVM predictions to 0/1 for metric calculation (1 for anomaly, 0 for normal)
    # Original prediction: -1 for anomaly, 1 for normal. So, -1 maps to 1 (anomaly), 1 maps to 0 (normal).
    ocsvm_predicted_labels = np.where(df_processed_original['ocsvm_anomaly_prediction'] == -1, 1, 0)
    ocsvm_true_labels = df_processed_original['true_label']

    ocsvm_precision = precision_score(ocsvm_true_labels, ocsvm_predicted_labels, average='binary', pos_label=1, zero_division=0)
    ocsvm_recall = recall_score(ocsvm_true_labels, ocsvm_predicted_labels, average='binary', pos_label=1, zero_division=0)
    ocsvm_f1 = f1_score(ocsvm_true_labels, ocsvm_predicted_labels, average='binary', pos_label=1, zero_division=0)

    print("\nOne-Class SVM (OCSVM) Performance:")
    print(f"  Precision: {ocsvm_precision:.4f}")
    print(f"  Recall: {ocsvm_recall:.4f}")
    print(f"  F1-Score: {ocsvm_f1:.4f}")

    # Evaluate Isolation Forest Performance
    # Convert Isolation Forest predictions to 0/1 for metric calculation
    if_predicted_labels = np.where(df_processed_original['if_anomaly_prediction'] == -1, 1, 0)
    if_true_labels = df_processed_original['true_label'] # Same true labels

    if_precision = precision_score(if_true_labels, if_predicted_labels, average='binary', pos_label=1, zero_division=0)
    if_recall = recall_score(if_true_labels, if_predicted_labels, average='binary', pos_label=1, zero_division=0)
    if_f1 = f1_score(if_true_labels, if_predicted_labels, average='binary', pos_label=1, zero_division=0)

    print("\nIsolation Forest Performance:")
    print(f"  Precision: {if_precision:.4f}")
    print(f"  Recall: {if_recall:.4f}")
    print(f"  F1-Score: {if_f1:.4f}")
    # <--- END OF THE MOVED SECTION!


    print("\n--- All Simulation and Visualization Complete ---")
